{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg1f8RnyZq1qhOqJl+aR61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Puthalir/Puthalir.github.io/blob/main/Day_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UChduyee1i8X",
        "outputId": "a9ec3912-e9cd-4c3c-e791-7852c77f26d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "3LBzrIq10NiH",
        "outputId": "1c08b3c9-13f5-49d9-c01f-67394bfefd19"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'driver' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4d48c644d69a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Indeed shows 10 jobs per page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{base_url}?q={query}&l={location}&start={page}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the page to load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Initialize the WebDriver\n",
        "#driver = webdriver.Chrome()  # or webdriver.Firefox() for Firefox\n",
        "\n",
        "# Base URL components\n",
        "base_url = \"https://www.indeed.com/jobs\"\n",
        "query = \"data+scientist\"\n",
        "location = \"Bolton%2C+MA\"\n",
        "\n",
        "# List to store job data\n",
        "job_listings = []\n",
        "\n",
        "# Iterate over the first 5 pages\n",
        "for page in range(0, 50, 10):  # Indeed shows 10 jobs per page\n",
        "    url = f\"{base_url}?q={query}&l={location}&start={page}\"\n",
        "    driver.get(url)\n",
        "    time.sleep(3)  # Wait for the page to load\n",
        "\n",
        "    # Find all job cards on the page\n",
        "    job_cards = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
        "\n",
        "    for card in job_cards:\n",
        "        try:\n",
        "            title = card.find_element(By.CLASS_NAME, 'jobTitle').text\n",
        "        except NoSuchElementException:\n",
        "            title = ''\n",
        "        try:\n",
        "            company = card.find_element(By.CLASS_NAME, 'companyName').text\n",
        "        except NoSuchElementException:\n",
        "            company = ''\n",
        "        try:\n",
        "            location = card.find_element(By.CLASS_NAME, 'companyLocation').text\n",
        "        except NoSuchElementException:\n",
        "            location = ''\n",
        "        try:\n",
        "            summary = card.find_element(By.CLASS_NAME, 'job-snippet').text\n",
        "        except NoSuchElementException:\n",
        "            summary = ''\n",
        "\n",
        "        job_listings.append({\n",
        "            'Title': title,\n",
        "            'Company': company,\n",
        "            'Location': location,\n",
        "            'Summary': summary\n",
        "        })\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df = pd.DataFrame(job_listings)\n",
        "df.to_csv('indeed_jobs.csv', index=False)\n",
        "print(\"Job listings have been saved to indeed_jobs.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "import tempfile\n",
        "\n",
        "# Setup Chrome options with a temporary profile to avoid session error\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")  # avoids profile conflict\n",
        "chrome_options.add_argument(\"--headless\")  # run in background\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Set path to chromedriver if not in PATH\n",
        "# Example: Service(\"/path/to/chromedriver\")\n",
        "service = Service()\n",
        "\n",
        "# Initialize the WebDriver\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Base URL\n",
        "base_url = \"https://www.indeed.com/jobs?q=data+scientist&l=Bolton%2C+MA&from=searchOnHP%2Cwhatautocomplete%2Cwhereautocomplete&vjk=2c15f6f156f07bdb\"\n",
        "query = \"data+scientist\"\n",
        "location = \"Bolton%2C+MA\"\n",
        "\n",
        "# List to store job data\n",
        "job_listings = []\n",
        "\n",
        "# Scrape first 5 pages (10 jobs per page)\n",
        "for page in range(0, 50, 10):\n",
        "    url = f\"{base_url}?q={query}&l={location}&start={page}\"\n",
        "    driver.get(url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    job_cards = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
        "\n",
        "    for card in job_cards:\n",
        "        try:\n",
        "            title = card.find_element(By.CLASS_NAME, 'jobTitle').text\n",
        "        except NoSuchElementException:\n",
        "            title = ''\n",
        "        try:\n",
        "            company = card.find_element(By.CLASS_NAME, 'companyName').text\n",
        "        except NoSuchElementException:\n",
        "            company = ''\n",
        "        try:\n",
        "            location = card.find_element(By.CLASS_NAME, 'companyLocation').text\n",
        "        except NoSuchElementException:\n",
        "            location = ''\n",
        "        try:\n",
        "            summary = card.find_element(By.CLASS_NAME, 'job-snippet').text\n",
        "        except NoSuchElementException:\n",
        "            summary = ''\n",
        "\n",
        "        job_listings.append({\n",
        "            'Title': title,\n",
        "            'Company': company,\n",
        "            'Location': location,\n",
        "            'Summary': summary\n",
        "        })\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(job_listings)\n",
        "df.to_csv('indeed_jobs.csv', index=False)\n",
        "print(\"Scraping complete. Saved to 'indeed_jobs.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPuOwJ9d1iHF",
        "outputId": "e1c42282-770b-4be7-d76f-f1f0df87e7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Saved to 'indeed_jobs.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Define headers to mimic a browser visit\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "# Base URL components\n",
        "base_url = \"https://www.indeed.com/jobs\"\n",
        "query_params = {\n",
        "    \"q\": \"data scientist\",\n",
        "    \"l\": \"Bolton, MA\",\n",
        "    \"start\": 0\n",
        "}\n",
        "\n",
        "# Open a file to write the job listings\n",
        "with open(\"indeed_jobs.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    for page in range(5):  # Iterate over the first 5 pages\n",
        "        query_params[\"start\"] = page * 10  # Indeed shows 10 jobs per page\n",
        "        response = requests.get(base_url, headers=headers, params=query_params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve page {page + 1}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        job_cards = soup.find_all(\"div\", class_=\"job_seen_beacon\")\n",
        "\n",
        "        for job in job_cards:\n",
        "            title_elem = job.find(\"h2\", class_=\"jobTitle\")\n",
        "            company_elem = job.find(\"span\", class_=\"companyName\")\n",
        "            location_elem = job.find(\"div\", class_=\"companyLocation\")\n",
        "\n",
        "            title = title_elem.get_text(strip=True) if title_elem else \"N/A\"\n",
        "            company = company_elem.get_text(strip=True) if company_elem else \"N/A\"\n",
        "            location = location_elem.get_text(strip=True) if location_elem else \"N/A\"\n",
        "\n",
        "            file.write(f\"Job Title: {title}\\n\")\n",
        "            file.write(f\"Company: {company}\\n\")\n",
        "            file.write(f\"Location: {location}\\n\")\n",
        "            file.write(\"-\" * 40 + \"\\n\")\n",
        "\n",
        "        print(f\"Page {page + 1} scraped successfully.\")\n",
        "        time.sleep(2)  # Delay to prevent overwhelming the server\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFlrHcVA8ehu",
        "outputId": "dbe13bda-cf47-4c72-ed2f-d1f742058c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve page 1\n",
            "Failed to retrieve page 2\n",
            "Failed to retrieve page 3\n",
            "Failed to retrieve page 4\n",
            "Failed to retrieve page 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eJLdDB6I_Io",
        "outputId": "3dbb722a-1a8f-44cf-bf44-85500b8fa14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def scrape_indeed_jobs(query, location, num_pages=5):\n",
        "    \"\"\"\n",
        "    Scrape Indeed job listings for a given query and location.\n",
        "\n",
        "    Args:\n",
        "        query (str): Job title or keyword to search for\n",
        "        location (str): Location to search in\n",
        "        num_pages (int): Number of pages to scrape (default: 5)\n",
        "\n",
        "    Returns:\n",
        "        list: List of job listings (each as a dictionary)\n",
        "    \"\"\"\n",
        "    # Format the search parameters for the URL\n",
        "    formatted_query = query.replace(' ', '+')\n",
        "    formatted_location = location.replace(' ', '+').replace(',', '%2C')\n",
        "\n",
        "    # List to store all job listings\n",
        "    all_jobs = []\n",
        "\n",
        "    # User-Agent to mimic a browser\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    # Loop through pages\n",
        "    for page in range(num_pages):\n",
        "        # Calculate the start parameter (10 jobs per page)\n",
        "        start = page * 10\n",
        "\n",
        "        # Create the URL for the current page\n",
        "        url = f\"https://www.indeed.com/jobs?q={formatted_query}&l={formatted_location}&start={start}\"\n",
        "\n",
        "        print(f\"Scraping page {page+1}/{num_pages}: {url}\")\n",
        "\n",
        "        try:\n",
        "            # Make the request\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "            # Check if request was successful\n",
        "            if response.status_code == 200:\n",
        "                # Parse the HTML\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Find all job cards\n",
        "                job_cards = soup.find_all('div', class_='job_seen_beacon')\n",
        "\n",
        "                if not job_cards:\n",
        "                    print(f\"No job cards found on page {page+1}. Indeed may have changed their HTML structure or is blocking scraping.\")\n",
        "                    continue\n",
        "\n",
        "                # Extract data from each job card\n",
        "                for card in job_cards:\n",
        "                    job_data = {}\n",
        "\n",
        "                    # Extract job title\n",
        "                    title_element = card.find('a', class_='jcs-JobTitle')\n",
        "                    if title_element:\n",
        "                        job_data['title'] = title_element.text.strip()\n",
        "\n",
        "                        # Extract job URL\n",
        "                        job_url = title_element.get('href')\n",
        "                        if job_url and job_url.startswith('/'):\n",
        "                            job_data['url'] = f\"https://www.indeed.com{job_url}\"\n",
        "                        else:\n",
        "                            job_data['url'] = job_url\n",
        "                    else:\n",
        "                        job_data['title'] = \"N/A\"\n",
        "                        job_data['url'] = \"N/A\"\n",
        "\n",
        "                    # Extract company name\n",
        "                    company_element = card.find('span', class_='companyName')\n",
        "                    job_data['company'] = company_element.text.strip() if company_element else \"N/A\"\n",
        "\n",
        "                    # Extract location\n",
        "                    location_element = card.find('div', class_='companyLocation')\n",
        "                    job_data['location'] = location_element.text.strip() if location_element else \"N/A\"\n",
        "\n",
        "                    # Extract salary if available\n",
        "                    salary_element = card.find('div', class_='metadata salary-snippet-container')\n",
        "                    job_data['salary'] = salary_element.text.strip() if salary_element else \"Not specified\"\n",
        "\n",
        "                    # Extract job snippet/description\n",
        "                    description_element = card.find('div', class_='job-snippet')\n",
        "                    job_data['description'] = description_element.text.strip() if description_element else \"N/A\"\n",
        "\n",
        "                    # Extract date posted\n",
        "                    date_element = card.find('span', class_='date')\n",
        "                    job_data['date_posted'] = date_element.text.strip() if date_element else \"N/A\"\n",
        "\n",
        "                    # Add to our list of jobs\n",
        "                    all_jobs.append(job_data)\n",
        "\n",
        "                # Add a delay between requests to avoid being blocked\n",
        "                time.sleep(random.uniform(2, 5))\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed to fetch page {page+1}. Status code: {response.status_code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping page {page+1}: {str(e)}\")\n",
        "\n",
        "    print(f\"Scraped a total of {len(all_jobs)} job listings.\")\n",
        "    return all_jobs\n",
        "\n",
        "def save_to_csv(jobs, filename=\"indeed_jobs.csv\"):\n",
        "    \"\"\"Save job listings to CSV file\"\"\"\n",
        "    if not jobs:\n",
        "        print(\"No jobs to save.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            # Get the keys from the first job to use as field names\n",
        "            fieldnames = jobs[0].keys()\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for job in jobs:\n",
        "                writer.writerow(job)\n",
        "\n",
        "        print(f\"Successfully saved {len(jobs)} jobs to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to CSV: {str(e)}\")\n",
        "\n",
        "def save_to_txt(jobs, filename=\"indeed_jobs.txt\"):\n",
        "    \"\"\"Save job listings to TXT file\"\"\"\n",
        "    if not jobs:\n",
        "        print(\"No jobs to save.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as txtfile:\n",
        "            txtfile.write(f\"INDEED JOB LISTINGS\\n\")\n",
        "            txtfile.write(f\"Total Jobs Found: {len(jobs)}\\n\")\n",
        "            txtfile.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "            for i, job in enumerate(jobs, 1):\n",
        "                txtfile.write(f\"Job #{i}\\n\")\n",
        "                txtfile.write(f\"Title: {job.get('title', 'N/A')}\\n\")\n",
        "                txtfile.write(f\"Company: {job.get('company', 'N/A')}\\n\")\n",
        "                txtfile.write(f\"Location: {job.get('location', 'N/A')}\\n\")\n",
        "                txtfile.write(f\"Salary: {job.get('salary', 'Not specified')}\\n\")\n",
        "                txtfile.write(f\"Date Posted: {job.get('date_posted', 'N/A')}\\n\")\n",
        "                txtfile.write(f\"Description: {job.get('description', 'N/A')}\\n\")\n",
        "                txtfile.write(f\"URL: {job.get('url', 'N/A')}\\n\")\n",
        "                txtfile.write(\"-\"*50 + \"\\n\\n\")\n",
        "\n",
        "        print(f\"Successfully saved {len(jobs)} jobs to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to TXT: {str(e)}\")\n",
        "\n",
        "def main():\n",
        "    # Get user input\n",
        "    query = input(\"Enter job title or keywords (default: 'data scientist'): \") or \"data scientist\"\n",
        "    location = input(\"Enter location (default: 'Bolton, MA'): \") or \"Bolton, MA\"\n",
        "    num_pages = int(input(\"Enter number of pages to scrape (default: 5): \") or 5)\n",
        "\n",
        "    # Scrape the jobs\n",
        "    jobs = scrape_indeed_jobs(query, location, num_pages)\n",
        "\n",
        "    if jobs:\n",
        "        # Save to both CSV and TXT formats\n",
        "        save_to_csv(jobs)\n",
        "        save_to_txt(jobs)\n",
        "    else:\n",
        "        print(\"No jobs were found or there was an issue with scraping.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "v1t1dVKgJBz-",
        "outputId": "5f07b732-3536-4835-e276-bfe90504ba36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-17-557a054a6150>, line 178)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-557a054a6150>\"\u001b[0;36m, line \u001b[0;32m178\u001b[0m\n\u001b[0;31m    python scrape_indeed_csv.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}